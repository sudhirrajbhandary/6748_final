{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f53f13-725d-4b62-9e29-d822c6a3102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters: {'classifier__C': 0.1, 'classifier__gamma': 'scale'}\n",
      "Best cross-validation score: 0.47\n",
      "Test Accuracy: 0.46891043924700515\n",
      "[[ 75  70 263]\n",
      " [ 16 216 432]\n",
      " [ 26 124 531]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Blockbuster       0.64      0.18      0.29       408\n",
      "        Flop       0.53      0.33      0.40       664\n",
      "     Success       0.43      0.78      0.56       681\n",
      "\n",
      "    accuracy                           0.47      1753\n",
      "   macro avg       0.53      0.43      0.41      1753\n",
      "weighted avg       0.52      0.47      0.44      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # For splitting data and hyperparameter tuning\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # For scaling numerical data and encoding categorical data\n",
    "from sklearn.compose import ColumnTransformer  # To combine transformations for different feature types\n",
    "from sklearn.pipeline import Pipeline  # To create a pipeline that chains data transformations and model fitting\n",
    "from sklearn.svm import SVC  # Support Vector Classifier (SVM)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # For model evaluation\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. Load the Data\n",
    "# -----------------------------------------\n",
    "# Read the combined dataset that includes a column 'dataset' indicating train, validation, or test split\n",
    "data = pd.read_csv('combined_df.csv')\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2. Define Features and Target\n",
    "# -----------------------------------------\n",
    "# List the features to be used for modeling and specify the target variable\n",
    "features = ['runtime', 'budget', 'genres', 'production_companies', 'production_countries', \n",
    "            'spoken_languages', 'director1', 'actor1Name']\n",
    "target = 'success_level'\n",
    "\n",
    "# Split the data into training, validation, and test sets based on the 'dataset' column\n",
    "train_data = data[data['dataset'] == 'train']\n",
    "validation_data = data[data['dataset'] == 'validation']\n",
    "test_data = data[data['dataset'] == 'test']\n",
    "\n",
    "# Separate features (X) and target (y) for each dataset\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_val = validation_data[features]\n",
    "y_val = validation_data[target]\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3. Preprocessing Pipelines\n",
    "# -----------------------------------------\n",
    "# Define the numerical features and create a pipeline for scaling them\n",
    "numeric_features = ['runtime', 'budget']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())  # Standardize numerical features by removing the mean and scaling to unit variance\n",
    "])\n",
    "\n",
    "# Define the categorical features and create a pipeline for one-hot encoding them\n",
    "categorical_features = ['genres', 'production_companies', 'production_countries', 'spoken_languages', \n",
    "                        'director1', 'actor1Name']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Convert categorical variables into a one-hot numeric array\n",
    "])\n",
    "\n",
    "# Combine the numerical and categorical pipelines into a single preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),      # Apply numeric_transformer to numeric_features\n",
    "        ('cat', categorical_transformer, categorical_features) # Apply categorical_transformer to categorical_features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. Create the SVM Model Pipeline\n",
    "# -----------------------------------------\n",
    "# Build a pipeline that first preprocesses the data and then fits a Support Vector Classifier with a linear kernel.\n",
    "svm_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Apply the combined preprocessing steps\n",
    "    ('classifier', SVC(kernel='linear', random_state=42))  # SVM classifier with a linear kernel and fixed random state\n",
    "])\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5. Hyperparameter Tuning via GridSearchCV\n",
    "# -----------------------------------------\n",
    "# Set up a parameter grid for tuning hyperparameters of the SVM model\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],  # Regularization parameter values\n",
    "    'classifier__gamma': ['scale', 'auto'],  # Gamma parameter for kernel function (included for experimentation)\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object to search for the best hyperparameters using 5-fold cross-validation,\n",
    "# optimizing for accuracy, and with verbosity to track progress.\n",
    "grid_search = GridSearchCV(svm_pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "# Fit the grid search on the validation data (you can also fit on training, but here we use validation)\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding cross-validation accuracy score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# -----------------------------------------\n",
    "# 6. Evaluate the SVM Model on Test Data\n",
    "# -----------------------------------------\n",
    "# Use the best estimator found by GridSearchCV to predict on the test set\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "# Print the test accuracy\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, y_test_pred)}')\n",
    "# Print the numeric confusion matrix and detailed classification report for test results\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf288a36-0fa9-464b-9072-44a4ea02aee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
